{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython import get_ipython\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import statistics as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/veetin/Desktop/drifttest_pa/main/dynamic_env.py:46: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  frequencies[label][action] = ast.literal_eval(frequencies[label][action]) #判断需要计算的内容是不是合法的Python类型，如果是则执行，否则就报错\n"
     ]
    }
   ],
   "source": [
    "from dynamic_env import TaskEnv_drift\n",
    "env = TaskEnv_drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drift happen\n",
      "['a0', 'a1', 'a2', 'a3', 'a4']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discrete(17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.set_flag()\n",
    "env.drift(add_states=5,add_actions=5)\n",
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveQLearningAgent:\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv_drift,\n",
    "                 exploration_rate: float = 0.1,\n",
    "                 learning_rate: float = 0.2,\n",
    "                 discount_factor: float = 1) -> None:\n",
    "        self.epsilon = exploration_rate\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # 使用字典替代固定numpy数组\n",
    "        self.q_table = {}\n",
    "        self.visit_counts = {}  # 跟踪状态-动作对的访问次数\n",
    "        \n",
    "        # 记录已知的状态和动作\n",
    "        self.known_states = set(env.states)\n",
    "        self.known_actions = set(env.motions)\n",
    "        \n",
    "        self.actions = env.motions\n",
    "        \n",
    "    def _get_q_value(self, state, action):\n",
    "        \"\"\"获取Q值，如果是新的状态-动作对则初始化\"\"\"\n",
    "        key = (state, action)\n",
    "        if key not in self.q_table:\n",
    "            self.q_table[key] = 0.0  # 可以调整初始值策略\n",
    "            self.visit_counts[key] = 0\n",
    "        return self.q_table[key]\n",
    "    \n",
    "    def _set_q_value(self, state, action, value):\n",
    "        \"\"\"设置Q值\"\"\"\n",
    "        key = (state, action)\n",
    "        self.q_table[key] = value\n",
    "        \n",
    "    def detect_new_state_action(self, state, available_actions):\n",
    "        \"\"\"检测和处理新的状态或动作\"\"\"\n",
    "        is_expanded = False\n",
    "        \n",
    "        # 检测新状态\n",
    "        if state not in self.known_states and state != 'Tau':\n",
    "            self.known_states.add(state)\n",
    "            is_expanded = True\n",
    "            \n",
    "        # 检测新动作\n",
    "        for action in self.actions:\n",
    "            if action not in self.known_actions:\n",
    "                self.known_actions.add(action)\n",
    "                is_expanded = True\n",
    "                \n",
    "        # 如果发生扩展，调整探索策略\n",
    "        if is_expanded:\n",
    "            self._adjust_exploration()\n",
    "            \n",
    "        return is_expanded\n",
    "    \n",
    "    def _adjust_exploration(self):\n",
    "        \"\"\"调整探索率\"\"\"\n",
    "        # 发现新状态或动作时暂时提高探索率\n",
    "        self.epsilon = min(0.5, self.epsilon * 1.5)\n",
    "        \n",
    "    def select_action(self, state, use_greedy_strategy: bool = False) -> int:\n",
    "        # 检测新状态和动作\n",
    "        self.detect_new_state_action(state, self.actions)\n",
    "        \n",
    "        if not use_greedy_strategy and random.random() < self.epsilon:\n",
    "            # 优先探索较少访问的动作\n",
    "            action_visits = {a: self.visit_counts.get((state, a), 0) \n",
    "                           for a in range(len(self.actions))}\n",
    "            min_visits = min(action_visits.values())\n",
    "            least_visited = [a for a, v in action_visits.items() \n",
    "                           if v == min_visits]\n",
    "            return np.random.choice(least_visited)\n",
    "            \n",
    "        # 获取所有动作的Q值\n",
    "        q_values = [self._get_q_value(state, a) for a in range(len(self.actions))]\n",
    "        max_val = max(q_values)\n",
    "        max_actions = [i for i, q in enumerate(q_values) if q == max_val]\n",
    "        return np.random.choice(max_actions)\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        # 更新访问计数\n",
    "        key = (state, action)\n",
    "        self.visit_counts[key] = self.visit_counts.get(key, 0) + 1\n",
    "        \n",
    "        # 获取下一状态的最大Q值\n",
    "        next_max_val = 0\n",
    "        if next_state != 'Tau':\n",
    "            # 检测新状态\n",
    "            self.detect_new_state_action(next_state, self.actions)\n",
    "            next_q_values = [self._get_q_value(next_state, a) \n",
    "                           for a in range(len(self.actions))]\n",
    "            next_max_val = max(next_q_values)\n",
    "            \n",
    "        # 更新Q值\n",
    "        current_q = self._get_q_value(state, action)\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * next_max_val - current_q)\n",
    "        self._set_q_value(state, action, new_q)\n",
    "        \n",
    "        # 动态调整学习参数\n",
    "        self._adjust_learning_parameters(state, action)\n",
    "        \n",
    "    def _adjust_learning_parameters(self, state, action):\n",
    "        \"\"\"根据访问次数动态调整学习参数\"\"\"\n",
    "        visits = self.visit_counts.get((state, action), 0)\n",
    "        \n",
    "        # 随访问次数降低学习率\n",
    "        self.alpha = max(0.01, 1.0 / (1 + visits * 0.1))\n",
    "        \n",
    "        # 如果所有状态-动作对都被充分访问，降低探索率\n",
    "        if min(self.visit_counts.values()) > 10:\n",
    "            self.epsilon = max(0.01, self.epsilon * 0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_control(add_states=2,add_actions=2, type_drif=None): #add other variable to control the type\n",
    "    \"\"\"which episode drift happen and which type\"\"\" \n",
    "    env.set_flag()\n",
    "    env.drift(add_states,add_actions)\n",
    "    #qlearner.change_qtable()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "def run_qlearner(nrofepisodes, gamma, epsilon = 0.1, alpha=0.2, train = True, agent = None, inform=False,drift_ep=0):\n",
    "    #add param control whether drift happen\n",
    "    if agent == None:\n",
    "        Q_learner = AdaptiveQLearningAgent(env, discount_factor=gamma, exploration_rate=epsilon, learning_rate=alpha)\n",
    "    else:\n",
    "        Q_learner = agent\n",
    "    state = env.reset()\n",
    "\n",
    "    information = {}\n",
    "    \n",
    "    rewards_ = []\n",
    "\n",
    "    for i in range(nrofepisodes):\n",
    "        terminated = False\n",
    "        sumRreward = 0\n",
    "        \n",
    "        if i == drift_ep and drift_ep!=0: #when at episode 10 data drift 只drift一次\n",
    "            print(\"whether drift happen at\",i,\"episode\")\n",
    "            drift_control()\n",
    "        while not terminated: \n",
    "            if train:\n",
    "                action = Q_learner.select_action(state)\n",
    "            else:\n",
    "                action = Q_learner.select_action(state, use_greedy_strategy=True)\n",
    "\n",
    "            if inform:\n",
    "                observation, reward, terminated, info = env.step(action, inform)\n",
    "                if info != []:\n",
    "                    key = ''.join(str(x) for x in info)\n",
    "                    if key not in information:\n",
    "                        information[key] = 1\n",
    "                    else:\n",
    "                        information[key] += 1\n",
    "            else:\n",
    "            # print(\"action index\", action)\n",
    "                observation, reward, terminated, info = env.step(action)    \n",
    "            \n",
    "            sumRreward += reward\n",
    "            \n",
    "            if train:\n",
    "                Q_learner.learn(state, action, observation, reward, terminated)    \n",
    "            \n",
    "            state = observation\n",
    "            i+=1\n",
    "            \n",
    "        \"\"\"if env.flag == True:\n",
    "            state = env.reset_drift()\n",
    "        else:\"\"\"\n",
    "        state = env.reset()\n",
    "        rewards_.append(sumRreward)\n",
    "    return Q_learner, rewards_, information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whether drift happen at 2 episode\n",
      "drift happen\n",
      "['a0', 'a1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent1_learner, agent1_reward, agent1_info = run_qlearner(10, 0.2, epsilon=0.1, alpha=0.2,drift_ep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['va', 'sib', 'pp', 'po', 'Tau', 's0', 's1'],\n",
       " [-1.0, 0.0, -19.0, -8.0, -6.0, -13.0, 0.0, -8.0, -9.0, 1.0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.states, agent1_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/veetin/Desktop/drifttest_pa/main/dynamic_env.py:46: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  frequencies[label][action] = ast.literal_eval(frequencies[label][action]) #判断需要计算的内容是不是合法的Python类型，如果是则执行，否则就报错\n"
     ]
    }
   ],
   "source": [
    "env2 = TaskEnv_drift()\n",
    "agent2_learner, agent2_reward, agent2_info = run_qlearner(10, 0.2, epsilon=0.1, alpha=0.2,drift_ep=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['va', 'sib', 'pp', 'po', 'Tau'],\n",
       " [0.0, 1.0, -4.0, 0.0, -3.0, 0.0, -12.0, -22.0, -9.0, -2.0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.states, agent2_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['va', 'sib', 'pp', 'po', 'Tau']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
